# version: '3.8'

# x-airflow-common:
#   &airflow-common
#   build:
#     context: ./airflow
#     dockerfile: Dockerfile
#   environment:
#     &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD}@postgres/airflow
#     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD}@postgres/airflow
#     AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
#     AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#     AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#     AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
#     AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
#     # Stock API Configuration
#     ALPHA_VANTAGE_API_KEY: ${ALPHA_VANTAGE_API_KEY}
#     STOCK_DB_HOST: postgres
#     STOCK_DB_NAME: ${STOCK_DB_NAME}
#     STOCK_DB_USER: ${STOCK_DB_USER}
#     STOCK_DB_PASSWORD: ${STOCK_DB_PASSWORD}
#     STOCK_DB_PORT: 5432
#     # Python path for custom modules
#     PYTHONPATH: /opt/airflow/dags:/opt/airflow/scripts
#   volumes:
#     - ./dags:/opt/airflow/dags
#     - ./scripts:/opt/airflow/scripts
#     - ./logs:/opt/airflow/logs
#   user: "${AIRFLOW_UID:-50000}:0"
#   depends_on:
#     &airflow-common-depends-on
#     redis:
#       condition: service_healthy
#     postgres:
#       condition: service_healthy

# # services:
# #   # PostgreSQL Database
# #   postgres:
# #     image: postgres:15
# #     environment:
# #       POSTGRES_USER: airflow
# #       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
# #       POSTGRES_DB: airflow
# #       # Create additional database for stock data
# #       POSTGRES_MULTIPLE_DATABASES: ${STOCK_DB_NAME}
# #     volumes:
# #       - postgres-db-volume:/var/lib/postgresql/data
# #       - ./sql/init.sql:/docker-entrypoint-initdb.d/01-init-stock-db.sql
# #       - ./sql/create-multiple-dbs.sh:/docker-entrypoint-initdb.d/00-create-multiple-dbs.sh
# #     healthcheck:
# #       test: ["CMD", "pg_isready", "-U", "airflow"]
# #       interval: 10s
# #       retries: 5
# #       start_period: 5s
# #     restart: always
# #     networks:
# #       - airflow-network

# # services:
# #   # PostgreSQL Database
# #   postgres:
# #     image: postgres:13
# #     container_name: stock_postgres
# #     environment:
# #       POSTGRES_USER: postgres
# #       POSTGRES_PASSWORD: mysecretpassword
# #       POSTGRES_DB: airflow 
# #       # Create additional database for stock data
# #       POSTGRES_MULTIPLE_DATABASES: ${STOCK_DB_NAME}
# #     volumes:
# #       - postgres-db-volume:/var/lib/postgresql/data
# #       # Custom init scripts
# #       - ./sql/create-multiple-dbs.sh:/docker-entrypoint-initdb.d/00-create-multiple-dbs.sh
# #       - ./sql/init.sql:/docker-entrypoint-initdb.d/01-init-stock-db.sql
# #     ports:
# #       - "5432:5432"
# #     healthcheck:
# #       test: ["CMD", "pg_isready", "-U", "airflow"]
# #       interval: 10s
# #       retries: 5
# #       start_period: 5s
# #     restart: always
# #     networks:
# #       - airflow-network

# # volumes:
# #   postgres-db-volume:

# # networks:
# #   airflow-network:
# #     driver: bridge








# # PostgreSQL Database
# services:
#   postgres:
#     image: postgres:13
#     container_name: postgres
#     environment:
#       POSTGRES_USER: postgres
#       POSTGRES_PASSWORD: mysecretpassword
#       #POSTGRES_USER: airflow
#       #POSTGRES_PASSWORD: airflow
#       POSTGRES_DB: airflow
#       #AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:mysecretpassword@postgres:5432/airflow

#       # Create additional database for stock data
#       POSTGRES_MULTIPLE_DATABASES: stock_data
#     volumes:
#       - postgres-db-volume:/var/lib/postgresql/data
#       # Custom init scripts
#       - ./sql/create-multiple-dbs.sh:/docker-entrypoint-initdb.d/00-create-multiple-dbs.sh
#       - ./sql/init.sql:/docker-entrypoint-initdb.d/01-init-stock-db.sql
#     ports:
#       - "5432:5432"
#     healthcheck:
#       test: ["CMD", "pg_isready", "-U", "postgres"]
#       interval: 10s
#       retries: 5
#       start_period: 5s
#     restart: always
#     networks:
#       - airflow-network




#   # Redis for Celery
#   redis:
#     image: redis:latest
#     expose:
#       - 6379
#     healthcheck:
#       test: ["CMD", "redis-cli", "ping"]
#       interval: 10s
#       timeout: 30s
#       retries: 50
#       start_period: 30s
#     restart: always
#     networks:
#       - airflow-network

#   # Airflow Webserver
#   airflow-webserver:
#     <<: *airflow-common
#     command: webserver
#     ports:
#       - "8080:8080"
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - airflow-network

#   # Airflow Scheduler
#   airflow-scheduler:
#     <<: *airflow-common
#     command: scheduler
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - airflow-network

#   # Airflow Worker
#   airflow-worker:
#     <<: *airflow-common
#     command: celery worker
#     healthcheck:
#       test:
#         - "CMD-SHELL"
#         - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     environment:
#       <<: *airflow-common-env
#       DUMB_INIT_SETSID: "0"
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - airflow-network

#   # Airflow Triggerer
#   airflow-triggerer:
#     <<: *airflow-common
#     command: triggerer
#     healthcheck:
#       test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - airflow-network

#   # Airflow Initialization
#   airflow-init:
#     <<: *airflow-common
#     entrypoint: /bin/bash
#     command:
#       - -c
#       - |
#         function ver() {
#           printf "%04d%04d%04d%04d" $${1//./ }
#         }
#         airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && airflow version)
#         airflow_version_comparable=$$(ver $${airflow_version})
#         min_airflow_version=2.2.0
#         min_airflow_version_comparable=$$(ver $${min_airflow_version})
#         if (( airflow_version_comparable < min_airflow_version_comparable )); then
#           echo
#           echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"
#           echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"
#           echo
#           exit 1
#         fi
#         if [[ -z "${AIRFLOW_UID}" ]]; then
#           echo
#           echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
#           echo "If you are on Linux, you SHOULD follow the instructions below to set "
#           echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
#           echo "For other operating systems you can get rid of the warning with manually created .env file:"
#           echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
#           echo
#         fi
#         one_meg=1048576
#         mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
#         cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
#         disk_available=$$(df / | tail -1 | awk '{print $$4}')
#         warning_resources="false"
#         if (( mem_available < 4000 )) ; then
#           echo
#           echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
#           echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
#           echo
#           warning_resources="true"
#         fi
#         if (( cpus_available < 2 )); then
#           echo
#           echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
#           echo "At least 2 CPUs recommended. You have $${cpus_available}"
#           echo
#           warning_resources="true"
#         fi
#         if (( disk_available < one_meg * 10 )); then
#           echo
#           echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
#           echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
#           echo
#           warning_resources="true"
#         fi
#         if [[ $${warning_resources} == "true" ]]; then
#           echo
#           echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
#           echo "Please follow the instructions to increase amount of resources available:"
#           echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
#           echo
#         fi
#         mkdir -p /sources/logs /sources/dags /sources/plugins
#         chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
#         exec /entrypoint airflow version
#     environment:
#       <<: *airflow-common-env
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow

#       _AIRFLOW_DB_UPGRADE: 'true'
#       _AIRFLOW_WWW_USER_CREATE: 'true'
#       _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
#       _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
#       _PIP_ADDITIONAL_REQUIREMENTS: ''
#     user: "0:0"
#     volumes:
#       - .:/sources
#     networks:
#       - airflow-network

#   # Flower for Celery monitoring (optional)
#   flower:
#     <<: *airflow-common
#     command: celery flower
#     ports:
#       - "5555:5555"
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-init:
#         condition: service_completed_successfully
#     networks:
#       - airflow-network
#   pgadmin:
#     image: dpage/pgadmin4
#     container_name: pgadmin
#     restart: always
#     environment:
#       PGADMIN_DEFAULT_EMAIL: admin@admin.com
#       PGADMIN_DEFAULT_PASSWORD: admin
#     ports:
#       - "5050:80"
#     depends_on:
#       - postgres
#     networks:
#       - airflow-network

# volumes:
#   postgres-db-volume:

# networks:
#   airflow-network:
#     driver: bridge













version: '3.8'

x-airflow-common:
  &airflow-common
  build:
    context: ./airflow
    dockerfile: Dockerfile
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-YOUR_GENERATED_KEY}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    ALPHA_VANTAGE_API_KEY: ${ALPHA_VANTAGE_API_KEY:-demo}
    STOCK_DB_HOST: postgres
    STOCK_DB_NAME: ${STOCK_DB_NAME:-stock_data}
    STOCK_DB_USER: airflow
    STOCK_DB_PASSWORD: airflow
    STOCK_DB_PORT: 5432
    PYTHONPATH: /opt/airflow/dags:/opt/airflow/scripts
  volumes:
    - ./dags:/opt/airflow/dags
    - ./scripts:/opt/airflow/scripts
    - ./logs:/opt/airflow/logs
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
      POSTGRES_MULTIPLE_DATABASES: stock_data
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
      - ./sql/create-multiple-dbs.sh:/docker-entrypoint-initdb.d/00-create-multiple-dbs.sh
      - ./sql/init.sql:/docker-entrypoint-initdb.d/01-init-stock-db.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - airflow-network

  redis:
    image: redis:latest
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always
    networks:
      - airflow-network

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    networks:
      - airflow-network

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    networks:
      - airflow-network

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    networks:
      - airflow-network

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    restart: always
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    networks:
      - airflow-network

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        exec /entrypoint airflow db upgrade && \
        airflow users create \
          --username airflow \
          --password airflow \
          --firstname airflow \
          --lastname airflow \
          --role Admin \
          --email airflow@airflow.com
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow
    user: "0:0"
    volumes:
      - .:/sources
    networks:
      - airflow-network

  flower:
    <<: *airflow-common
    command: celery flower
    ports:
      - "5555:5555"
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - airflow-network

  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - airflow-network

volumes:
  postgres-db-volume:

networks:
  airflow-network:
    driver: bridge



















